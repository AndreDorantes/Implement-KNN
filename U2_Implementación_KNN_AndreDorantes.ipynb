{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Machine Learning\n",
        "\n",
        "# U2 - Implementación KNN\n",
        "\n",
        "# Oscar Andre Dorantes Victor\n",
        "\n",
        "# IRC 9B\n",
        "\n",
        "# 10/9/2023"
      ],
      "metadata": {
        "id": "bkZeEoAswiDy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explicación de la intuición detrás del algoritmo de KNN.**\n",
        "\n",
        "We have a scatter plot of different colored points on a plane, with each color representing a different category. If a new uncolored point appears on this plane, how could you determine its color? One intuitive way is to look at the colors of the points that are closest to it. If most of the nearest points are red, it would make sense to classify the new point as red as well. That’s the basic idea behind the KNN algorithm.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZsMNbbJFwsLN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pseudocódigo del algoritmo**\n"
      ],
      "metadata": {
        "id": "_qiiii8qxERU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Initialize the training data and labels\n",
        "\n",
        "    X_train <- [[1, 2], [2, 3], [3, 3], [6, 6], [7, 7]]\n",
        "\n",
        "    y_train <- [0, 0, 0, 1, 1]\n",
        "\n",
        "\n",
        "\n",
        "2. Initialize the new data point to be classified\n",
        "\n",
        "    X_new <- [5, 5]\n",
        "\n",
        "\n",
        "\n",
        "3. Calculate the Euclidean distances from X_new to every point in X_train\n",
        "\n",
        "    FOR each point in X_train\n",
        "\n",
        "     CALCULATE the Euclidean distance to X_new\n",
        "\n",
        "    ENDFOR\n",
        "    \n",
        "\n",
        "\n",
        "4. Sort the distances and get the indices of the k closest points\n",
        "\n",
        "    k <- 3\n",
        "\n",
        "    k_indices <- GET the indices of the k smallest distances\n",
        "\n",
        "\n",
        "\n",
        "5. Fetch the labels corresponding to the k closest points\n",
        "\n",
        "    k_labels <- GET labels from y_train corresponding to k_indices\n",
        "\n",
        "\n",
        "\n",
        "6. Count the occurrences of each unique label\n",
        "\n",
        "    COUNT the occurrences of each unique label in k_labels\n",
        "\n",
        "\n",
        "\n",
        "7. Determine the label with the maximum count - that’s the majority vote\n",
        "\n",
        "    majority_vote <- LABEL with the maximum count\n",
        "\n",
        "\n",
        "\n",
        "8. Print the results\n",
        "\n",
        "    PRINT the calculated distances\n",
        "\n",
        "    PRINT k_indices\n",
        "\n",
        "    PRINT k_labels\n",
        "\n",
        "    PRINT the class that the new point is predicted to belong to,\n",
        "\n",
        "    which is the majority_vote"
      ],
      "metadata": {
        "id": "DWrZUG3ajMIN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implementación del algoritmo (propia) en Python (Jupyter Notebooks preferably).**\n",
        "\n"
      ],
      "metadata": {
        "id": "7c9-OXHyxEbu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "#Training data and labels\n",
        "X_train = np.array([[1, 2], [2, 3], [3, 3], [6, 6], [7, 7]])\n",
        "y_train = np.array([0, 0, 0, 1, 1])\n",
        "\n",
        "#New data point\n",
        "X_new = np.array([5, 5])\n",
        "\n",
        "#Calculate Euclidean distances\n",
        "distances = np.sqrt(np.sum((X_train - X_new)**2, axis=1))\n",
        "\n",
        "#Number of neighbors to consider\n",
        "k = 3\n",
        "\n",
        "#Get indices of k closest points\n",
        "k_indices = np.argsort(distances)[:k]\n",
        "\n",
        "#Fetch labels of k closest points\n",
        "k_labels = y_train[k_indices]\n",
        "\n",
        "#Count unique labels to determine majority\n",
        "unique_labels, counts = np.unique(k_labels, return_counts=True)\n",
        "\n",
        "#Majority voting\n",
        "majority_vote = unique_labels[np.argmax(counts)]\n",
        "\n",
        "#Print results\n",
        "print(\"Distances: \", distances)\n",
        "print(\"Indices of 3 closest points: \", k_indices)\n",
        "print(\"Labels of 3 closest points: \", k_labels)\n",
        "print(\"New point is predicted to belong to class: \", majority_vote)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7gCTPuHg3w_j",
        "outputId": "3448abeb-0b24-4560-a041-7fa6fbfa9163"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distances:  [5.         3.60555128 2.82842712 1.41421356 2.82842712]\n",
            "Indices of 3 closest points:  [3 2 4]\n",
            "Labels of 3 closest points:  [1 0 1]\n",
            "New point is predicted to belong to class:  1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loss function + Optimization function identification.**\n",
        "\n",
        "1. KNN Nature:\n",
        "KNN is an instance-based learning algorithm. It doesn't build a predictive model and makes decisions in the moment, using the closest data points from the training dataset.\n",
        "\n",
        "2. No Loss Function:\n",
        "There's no loss function in KNN because it doesn't have parameters to optimize. It directly uses training data for predictions, eliminating the need for error minimization.\n",
        "\n",
        "3. No Optimization Process:\n",
        "KNN lacks an optimization process because it doesn’t train a model. Predictions are made via a simple majority vote or averaging of the nearest neighbors, without tuning parameters."
      ],
      "metadata": {
        "id": "bo8I6K_gxKCD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**References**\n",
        "\n",
        "[1] T. Srivastava, “A complete guide to K-Nearest neighbors (Updated 2023),” Analytics Vidhya, Sep. 2023, [Online]. Available: https://www.analyticsvidhya.com/blog/2018/03/introduction-k-neighbours-algorithm-clustering/#:~:text=The%20K-Nearest%20Neighbor%20(KNN,training%20dataset%20as%20a%20reference.\n",
        "\n",
        "[2] “What is the k-nearest neighbors algorithm? | IBM.” https://www.ibm.com/topics/knn\n",
        "\n",
        "[3] O. Harrison, “Machine Learning Basics with the K-Nearest Neighbors Algorithm,” Medium, Jul. 14, 2019. [Online]. Available: https://towardsdatascience.com/machine-learning-basics-with-the-k-nearest-neighbors-algorithm-6a6e71d01761"
      ],
      "metadata": {
        "id": "ybliZEkihU5N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!jupyter nbconvert --to html \"U2_Implementación_KNN_AndreDorantes.ipynb\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCBRzffolWeV",
        "outputId": "db667867-ecbb-48b8-e966-c5951b885df2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NbConvertApp] Converting notebook U2_Implementación_KNN_AndreDorantes.ipynb to html\n",
            "[NbConvertApp] Writing 586269 bytes to U2_Implementación_KNN_AndreDorantes.html\n"
          ]
        }
      ]
    }
  ]
}